{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project Part 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "\n",
    "import pandas as pd\n",
    "import torch \n",
    "from torch.utils.data import Dataset, random_split\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, TrainingArguments, Trainer\n",
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration cakiki--gutenberg-poetry-corpus-7745b6aecdad34dc\n",
      "Found cached dataset parquet (C:/Users/Shayne Kaiser/.cache/huggingface/datasets/biglam___parquet/cakiki--gutenberg-poetry-corpus-7745b6aecdad34dc/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec)\n"
     ]
    }
   ],
   "source": [
    "# load dataset\n",
    "dataset = load_dataset(\"biglam/gutenberg-poetry-corpus\", split=\"train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['line', 'gutenberg_id'],\n",
       "    num_rows: 3085117\n",
       "})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert to Pandas\n",
    "df = pd.DataFrame(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df shape: (3085117, 2)\n"
     ]
    }
   ],
   "source": [
    "print('df shape:', df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    The Song of Hiawatha is based on the legends a...\n",
       "1    many North American Indian tribes, but especia...\n",
       "2    Ojibway Indians of northern Michigan, Wisconsi...\n",
       "3    They were collected by Henry Rowe Schoolcraft,...\n",
       "4    Schoolcraft married Jane, O-bah-bahm-wawa-ge-z...\n",
       "Name: line, dtype: object"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lines = df[\"line\"]\n",
    "lines.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_length = max([len(tokenizer.encode(line)) for line in lines])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "85"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PoetryDataset(Dataset):\n",
    "    def __init__(self, txt_list, tokenizer, max_length):\n",
    "        self.input_ids = []\n",
    "        self.attn_masks = []\n",
    "        self.labels = []\n",
    "        for txt in txt_list:\n",
    "            encodings_dict = tokenizer('<|startoftext|>' + txt + '<|endoftext|>', truncation=True,\n",
    "                                       max_length=max_length, padding=\"max_length\")\n",
    "            self.input_ids.append(torch.tensor(encodings_dict['input_ids']))\n",
    "            self.attn_masks.append(torch.tensor(encodings_dict['attention_mask']))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.input_ids)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.input_ids[idx], self.attn_masks[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenization_dataset = PoetryDataset(lines, tokenizer, max_length=max_length)\n",
    "train_size = int(0.9  * len(tokenization_dataset))\n",
    "train_dataset, val_dataset = random_split(tokenization_dataset, [train_size, len(tokenization_dataset) - train_size])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([50257,   818,   262,  7032,   262,   302,   521, 28153,  4836,   606,\n",
       "         11496, 50256, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258,\n",
       "         50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258,\n",
       "         50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258,\n",
       "         50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258,\n",
       "         50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258,\n",
       "         50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258,\n",
       "         50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258,\n",
       "         50258, 50258, 50258, 50258, 50258]),\n",
       " tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of Training dataset: 2776605\n",
      "Length of Validation dataset: 308512\n"
     ]
    }
   ],
   "source": [
    "print(f\"Length of Training dataset: {len(train_dataset)}\")\n",
    "print(f\"Length of Validation dataset: {len(val_dataset)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DistilGPT2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Embedding(50259, 768)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load in model\n",
    "torch.manual_seed(92)\n",
    "\n",
    "MODEL_NAME = \"distilgpt2\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, bos_token='<|startoftext|>', eos_token='<|endoftext|>', pad_token='<|pad|>')\n",
    "model = AutoModelForCausalLM.from_pretrained(MODEL_NAME).cuda()\n",
    "model.resize_token_embeddings(len(tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(output_dir='./Models/DistilGPT2', num_train_epochs=1, logging_steps=10000, save_steps=500000,\n",
    "                                  per_device_train_batch_size=10, per_device_eval_batch_size=10, warmup_steps=10,\n",
    "                                   weight_decay=0.05, logging_dir='./Models/DistilGPT2/logs', report_to='none' )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Freezing layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "transformer.wte.weight True\n",
      "transformer.wpe.weight True\n",
      "transformer.h.0.ln_1.weight True\n",
      "transformer.h.0.ln_1.bias True\n",
      "transformer.h.0.attn.c_attn.weight True\n",
      "transformer.h.0.attn.c_attn.bias True\n",
      "transformer.h.0.attn.c_proj.weight True\n",
      "transformer.h.0.attn.c_proj.bias True\n",
      "transformer.h.0.ln_2.weight True\n",
      "transformer.h.0.ln_2.bias True\n",
      "transformer.h.0.mlp.c_fc.weight True\n",
      "transformer.h.0.mlp.c_fc.bias True\n",
      "transformer.h.0.mlp.c_proj.weight True\n",
      "transformer.h.0.mlp.c_proj.bias True\n",
      "transformer.h.1.ln_1.weight True\n",
      "transformer.h.1.ln_1.bias True\n",
      "transformer.h.1.attn.c_attn.weight True\n",
      "transformer.h.1.attn.c_attn.bias True\n",
      "transformer.h.1.attn.c_proj.weight True\n",
      "transformer.h.1.attn.c_proj.bias True\n",
      "transformer.h.1.ln_2.weight True\n",
      "transformer.h.1.ln_2.bias True\n",
      "transformer.h.1.mlp.c_fc.weight True\n",
      "transformer.h.1.mlp.c_fc.bias True\n",
      "transformer.h.1.mlp.c_proj.weight True\n",
      "transformer.h.1.mlp.c_proj.bias True\n",
      "transformer.h.2.ln_1.weight True\n",
      "transformer.h.2.ln_1.bias True\n",
      "transformer.h.2.attn.c_attn.weight True\n",
      "transformer.h.2.attn.c_attn.bias True\n",
      "transformer.h.2.attn.c_proj.weight True\n",
      "transformer.h.2.attn.c_proj.bias True\n",
      "transformer.h.2.ln_2.weight True\n",
      "transformer.h.2.ln_2.bias True\n",
      "transformer.h.2.mlp.c_fc.weight True\n",
      "transformer.h.2.mlp.c_fc.bias True\n",
      "transformer.h.2.mlp.c_proj.weight True\n",
      "transformer.h.2.mlp.c_proj.bias True\n",
      "transformer.h.3.ln_1.weight True\n",
      "transformer.h.3.ln_1.bias True\n",
      "transformer.h.3.attn.c_attn.weight True\n",
      "transformer.h.3.attn.c_attn.bias True\n",
      "transformer.h.3.attn.c_proj.weight True\n",
      "transformer.h.3.attn.c_proj.bias True\n",
      "transformer.h.3.ln_2.weight True\n",
      "transformer.h.3.ln_2.bias True\n",
      "transformer.h.3.mlp.c_fc.weight True\n",
      "transformer.h.3.mlp.c_fc.bias True\n",
      "transformer.h.3.mlp.c_proj.weight True\n",
      "transformer.h.3.mlp.c_proj.bias True\n",
      "transformer.h.4.ln_1.weight True\n",
      "transformer.h.4.ln_1.bias True\n",
      "transformer.h.4.attn.c_attn.weight True\n",
      "transformer.h.4.attn.c_attn.bias True\n",
      "transformer.h.4.attn.c_proj.weight True\n",
      "transformer.h.4.attn.c_proj.bias True\n",
      "transformer.h.4.ln_2.weight True\n",
      "transformer.h.4.ln_2.bias True\n",
      "transformer.h.4.mlp.c_fc.weight True\n",
      "transformer.h.4.mlp.c_fc.bias True\n",
      "transformer.h.4.mlp.c_proj.weight True\n",
      "transformer.h.4.mlp.c_proj.bias True\n",
      "transformer.h.5.ln_1.weight True\n",
      "transformer.h.5.ln_1.bias True\n",
      "transformer.h.5.attn.c_attn.weight True\n",
      "transformer.h.5.attn.c_attn.bias True\n",
      "transformer.h.5.attn.c_proj.weight True\n",
      "transformer.h.5.attn.c_proj.bias True\n",
      "transformer.h.5.ln_2.weight True\n",
      "transformer.h.5.ln_2.bias True\n",
      "transformer.h.5.mlp.c_fc.weight True\n",
      "transformer.h.5.mlp.c_fc.bias True\n",
      "transformer.h.5.mlp.c_proj.weight True\n",
      "transformer.h.5.mlp.c_proj.bias True\n",
      "transformer.ln_f.weight True\n",
      "transformer.ln_f.bias True\n"
     ]
    }
   ],
   "source": [
    "# check layers in the model\n",
    "for name, param in model.named_parameters():\n",
    "    print(name, param.requires_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# freeze the first two layers and 4 hidden units\n",
    "for name, param in model.named_parameters():\n",
    "    if name.startswith(\"transformer.wte\"):\n",
    "        param.requires_grad = False\n",
    "    if name.startswith(\"transformer.wpe\"):\n",
    "        param.requires_grad = False\n",
    "    if any(x in name for x in ['.' + str(x) + '.' for x in range(5)]):\n",
    "        param.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "transformer.wte.weight False\n",
      "transformer.wpe.weight False\n",
      "transformer.h.0.ln_1.weight False\n",
      "transformer.h.0.ln_1.bias False\n",
      "transformer.h.0.attn.c_attn.weight False\n",
      "transformer.h.0.attn.c_attn.bias False\n",
      "transformer.h.0.attn.c_proj.weight False\n",
      "transformer.h.0.attn.c_proj.bias False\n",
      "transformer.h.0.ln_2.weight False\n",
      "transformer.h.0.ln_2.bias False\n",
      "transformer.h.0.mlp.c_fc.weight False\n",
      "transformer.h.0.mlp.c_fc.bias False\n",
      "transformer.h.0.mlp.c_proj.weight False\n",
      "transformer.h.0.mlp.c_proj.bias False\n",
      "transformer.h.1.ln_1.weight False\n",
      "transformer.h.1.ln_1.bias False\n",
      "transformer.h.1.attn.c_attn.weight False\n",
      "transformer.h.1.attn.c_attn.bias False\n",
      "transformer.h.1.attn.c_proj.weight False\n",
      "transformer.h.1.attn.c_proj.bias False\n",
      "transformer.h.1.ln_2.weight False\n",
      "transformer.h.1.ln_2.bias False\n",
      "transformer.h.1.mlp.c_fc.weight False\n",
      "transformer.h.1.mlp.c_fc.bias False\n",
      "transformer.h.1.mlp.c_proj.weight False\n",
      "transformer.h.1.mlp.c_proj.bias False\n",
      "transformer.h.2.ln_1.weight False\n",
      "transformer.h.2.ln_1.bias False\n",
      "transformer.h.2.attn.c_attn.weight False\n",
      "transformer.h.2.attn.c_attn.bias False\n",
      "transformer.h.2.attn.c_proj.weight False\n",
      "transformer.h.2.attn.c_proj.bias False\n",
      "transformer.h.2.ln_2.weight False\n",
      "transformer.h.2.ln_2.bias False\n",
      "transformer.h.2.mlp.c_fc.weight False\n",
      "transformer.h.2.mlp.c_fc.bias False\n",
      "transformer.h.2.mlp.c_proj.weight False\n",
      "transformer.h.2.mlp.c_proj.bias False\n",
      "transformer.h.3.ln_1.weight False\n",
      "transformer.h.3.ln_1.bias False\n",
      "transformer.h.3.attn.c_attn.weight False\n",
      "transformer.h.3.attn.c_attn.bias False\n",
      "transformer.h.3.attn.c_proj.weight False\n",
      "transformer.h.3.attn.c_proj.bias False\n",
      "transformer.h.3.ln_2.weight False\n",
      "transformer.h.3.ln_2.bias False\n",
      "transformer.h.3.mlp.c_fc.weight False\n",
      "transformer.h.3.mlp.c_fc.bias False\n",
      "transformer.h.3.mlp.c_proj.weight False\n",
      "transformer.h.3.mlp.c_proj.bias False\n",
      "transformer.h.4.ln_1.weight False\n",
      "transformer.h.4.ln_1.bias False\n",
      "transformer.h.4.attn.c_attn.weight False\n",
      "transformer.h.4.attn.c_attn.bias False\n",
      "transformer.h.4.attn.c_proj.weight False\n",
      "transformer.h.4.attn.c_proj.bias False\n",
      "transformer.h.4.ln_2.weight False\n",
      "transformer.h.4.ln_2.bias False\n",
      "transformer.h.4.mlp.c_fc.weight False\n",
      "transformer.h.4.mlp.c_fc.bias False\n",
      "transformer.h.4.mlp.c_proj.weight False\n",
      "transformer.h.4.mlp.c_proj.bias False\n",
      "transformer.h.5.ln_1.weight True\n",
      "transformer.h.5.ln_1.bias True\n",
      "transformer.h.5.attn.c_attn.weight True\n",
      "transformer.h.5.attn.c_attn.bias True\n",
      "transformer.h.5.attn.c_proj.weight True\n",
      "transformer.h.5.attn.c_proj.bias True\n",
      "transformer.h.5.ln_2.weight True\n",
      "transformer.h.5.ln_2.bias True\n",
      "transformer.h.5.mlp.c_fc.weight True\n",
      "transformer.h.5.mlp.c_fc.bias True\n",
      "transformer.h.5.mlp.c_proj.weight True\n",
      "transformer.h.5.mlp.c_proj.bias True\n",
      "transformer.ln_f.weight True\n",
      "transformer.ln_f.bias True\n"
     ]
    }
   ],
   "source": [
    "# now check layers\n",
    "for name, param in model.named_parameters():\n",
    "    print(name, param.requires_grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "Trainer(model=model,  args=training_args, train_dataset=train_dataset, \n",
    "        eval_dataset=val_dataset, data_collator=lambda data: {'input_ids': torch.stack([f[0] for f in data]),\n",
    "                                                              'attention_mask': torch.stack([f[1] for f in data]),\n",
    "                                                              'labels': torch.stack([f[0] for f in data])}).train()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Configuration saved in ./Models/DistilGPT2\\config.json\n",
      "Model weights saved in ./Models/DistilGPT2\\pytorch_model.bin\n"
     ]
    }
   ],
   "source": [
    "# Save model and Tokenizer\n",
    "model.save_pretrained(\"./Models/DistilGPT2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "tokenizer config file saved in ./Models/DistilGPT2\\tokenizer_config.json\n",
      "Special tokens file saved in ./Models/DistilGPT2\\special_tokens_map.json\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('./Models/DistilGPT2\\\\tokenizer_config.json',\n",
       " './Models/DistilGPT2\\\\special_tokens_map.json',\n",
       " './Models/DistilGPT2\\\\vocab.json',\n",
       " './Models/DistilGPT2\\\\merges.txt',\n",
       " './Models/DistilGPT2\\\\added_tokens.json',\n",
       " './Models/DistilGPT2\\\\tokenizer.json')"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.save_pretrained(\"./Models/DistilGPT2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load in saved model and tokenizer \n",
    "tokenizer = AutoTokenizer.from_pretrained(\"./Models/DistilGPT2\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\"./Models/DistilGPT2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Is CUDA supported by this system? True\n",
      "CUDA version: 11.7\n",
      "ID of current CUDA device: 0\n",
      "Name of current CUDA device: NVIDIA GeForce RTX 3070\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "print(f\"Is CUDA supported by this system? {torch.cuda.is_available()}\")\n",
    "print(f\"CUDA version: {torch.version.cuda}\")\n",
    "\n",
    "# Storing ID of current CUDA device\n",
    "cuda_id = torch.cuda.current_device()\n",
    "print(f\"ID of current CUDA device: {torch.cuda.current_device()}\")\n",
    "\t\t\n",
    "print(f\"Name of current CUDA device: {torch.cuda.get_device_name(cuda_id)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GPT-Neo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Embedding(50259, 2048)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load in model\n",
    "torch.manual_seed(92)\n",
    "\n",
    "MODEL_NAME = \"EleutherAI/gpt-neo-1.3B\"\n",
    "\n",
    "tokenizer_neo = AutoTokenizer.from_pretrained(MODEL_NAME, bos_token='<|startoftext|>', eos_token='<|endoftext|>', pad_token='<|pad|>')\n",
    "model_neo = AutoModelForCausalLM.from_pretrained(MODEL_NAME).cuda()\n",
    "model_neo.resize_token_embeddings(len(tokenizer_neo))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Traing arguments\n",
    "training_args_neo = TrainingArguments(output_dir='./Models/GPT-Neo', num_train_epochs=1, logging_steps=10000, save_steps=500000,\n",
    "                                  per_device_train_batch_size=10, per_device_eval_batch_size=10, warmup_steps=10,\n",
    "                                   weight_decay=0.05, logging_dir='./Models/GPT-Neo/logs', report_to='none' )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenize data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_length = max([len(tokenizer_neo.encode(line)) for line in lines])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "85"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenization_dataset = PoetryDataset(lines, tokenizer_neo, max_length=max_length)\n",
    "train_size = int(0.9  * len(tokenization_dataset))\n",
    "train_dataset_neo, val_dataset_neo = random_split(tokenization_dataset, [train_size, len(tokenization_dataset) - train_size])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([50257,  3152,   257,  9480,  2786,   273,    11,   543,    11,   996,\n",
       "           284,   262,  4151, 50256, 50258, 50258, 50258, 50258, 50258, 50258,\n",
       "         50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258,\n",
       "         50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258,\n",
       "         50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258,\n",
       "         50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258,\n",
       "         50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258,\n",
       "         50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258,\n",
       "         50258, 50258, 50258, 50258, 50258]),\n",
       " tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset_neo[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of Training dataset: 2776605\n",
      "Length of Validation dataset: 308512\n"
     ]
    }
   ],
   "source": [
    "print(f\"Length of Training dataset: {len(train_dataset_neo)}\")\n",
    "print(f\"Length of Validation dataset: {len(val_dataset_neo)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Freeze layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "transformer.wte.weight True\n",
      "transformer.wpe.weight True\n",
      "transformer.h.0.ln_1.weight True\n",
      "transformer.h.0.ln_1.bias True\n",
      "transformer.h.0.attn.attention.k_proj.weight True\n",
      "transformer.h.0.attn.attention.v_proj.weight True\n",
      "transformer.h.0.attn.attention.q_proj.weight True\n",
      "transformer.h.0.attn.attention.out_proj.weight True\n",
      "transformer.h.0.attn.attention.out_proj.bias True\n",
      "transformer.h.0.ln_2.weight True\n",
      "transformer.h.0.ln_2.bias True\n",
      "transformer.h.0.mlp.c_fc.weight True\n",
      "transformer.h.0.mlp.c_fc.bias True\n",
      "transformer.h.0.mlp.c_proj.weight True\n",
      "transformer.h.0.mlp.c_proj.bias True\n",
      "transformer.h.1.ln_1.weight True\n",
      "transformer.h.1.ln_1.bias True\n",
      "transformer.h.1.attn.attention.k_proj.weight True\n",
      "transformer.h.1.attn.attention.v_proj.weight True\n",
      "transformer.h.1.attn.attention.q_proj.weight True\n",
      "transformer.h.1.attn.attention.out_proj.weight True\n",
      "transformer.h.1.attn.attention.out_proj.bias True\n",
      "transformer.h.1.ln_2.weight True\n",
      "transformer.h.1.ln_2.bias True\n",
      "transformer.h.1.mlp.c_fc.weight True\n",
      "transformer.h.1.mlp.c_fc.bias True\n",
      "transformer.h.1.mlp.c_proj.weight True\n",
      "transformer.h.1.mlp.c_proj.bias True\n",
      "transformer.h.2.ln_1.weight True\n",
      "transformer.h.2.ln_1.bias True\n",
      "transformer.h.2.attn.attention.k_proj.weight True\n",
      "transformer.h.2.attn.attention.v_proj.weight True\n",
      "transformer.h.2.attn.attention.q_proj.weight True\n",
      "transformer.h.2.attn.attention.out_proj.weight True\n",
      "transformer.h.2.attn.attention.out_proj.bias True\n",
      "transformer.h.2.ln_2.weight True\n",
      "transformer.h.2.ln_2.bias True\n",
      "transformer.h.2.mlp.c_fc.weight True\n",
      "transformer.h.2.mlp.c_fc.bias True\n",
      "transformer.h.2.mlp.c_proj.weight True\n",
      "transformer.h.2.mlp.c_proj.bias True\n",
      "transformer.h.3.ln_1.weight True\n",
      "transformer.h.3.ln_1.bias True\n",
      "transformer.h.3.attn.attention.k_proj.weight True\n",
      "transformer.h.3.attn.attention.v_proj.weight True\n",
      "transformer.h.3.attn.attention.q_proj.weight True\n",
      "transformer.h.3.attn.attention.out_proj.weight True\n",
      "transformer.h.3.attn.attention.out_proj.bias True\n",
      "transformer.h.3.ln_2.weight True\n",
      "transformer.h.3.ln_2.bias True\n",
      "transformer.h.3.mlp.c_fc.weight True\n",
      "transformer.h.3.mlp.c_fc.bias True\n",
      "transformer.h.3.mlp.c_proj.weight True\n",
      "transformer.h.3.mlp.c_proj.bias True\n",
      "transformer.h.4.ln_1.weight True\n",
      "transformer.h.4.ln_1.bias True\n",
      "transformer.h.4.attn.attention.k_proj.weight True\n",
      "transformer.h.4.attn.attention.v_proj.weight True\n",
      "transformer.h.4.attn.attention.q_proj.weight True\n",
      "transformer.h.4.attn.attention.out_proj.weight True\n",
      "transformer.h.4.attn.attention.out_proj.bias True\n",
      "transformer.h.4.ln_2.weight True\n",
      "transformer.h.4.ln_2.bias True\n",
      "transformer.h.4.mlp.c_fc.weight True\n",
      "transformer.h.4.mlp.c_fc.bias True\n",
      "transformer.h.4.mlp.c_proj.weight True\n",
      "transformer.h.4.mlp.c_proj.bias True\n",
      "transformer.h.5.ln_1.weight True\n",
      "transformer.h.5.ln_1.bias True\n",
      "transformer.h.5.attn.attention.k_proj.weight True\n",
      "transformer.h.5.attn.attention.v_proj.weight True\n",
      "transformer.h.5.attn.attention.q_proj.weight True\n",
      "transformer.h.5.attn.attention.out_proj.weight True\n",
      "transformer.h.5.attn.attention.out_proj.bias True\n",
      "transformer.h.5.ln_2.weight True\n",
      "transformer.h.5.ln_2.bias True\n",
      "transformer.h.5.mlp.c_fc.weight True\n",
      "transformer.h.5.mlp.c_fc.bias True\n",
      "transformer.h.5.mlp.c_proj.weight True\n",
      "transformer.h.5.mlp.c_proj.bias True\n",
      "transformer.h.6.ln_1.weight True\n",
      "transformer.h.6.ln_1.bias True\n",
      "transformer.h.6.attn.attention.k_proj.weight True\n",
      "transformer.h.6.attn.attention.v_proj.weight True\n",
      "transformer.h.6.attn.attention.q_proj.weight True\n",
      "transformer.h.6.attn.attention.out_proj.weight True\n",
      "transformer.h.6.attn.attention.out_proj.bias True\n",
      "transformer.h.6.ln_2.weight True\n",
      "transformer.h.6.ln_2.bias True\n",
      "transformer.h.6.mlp.c_fc.weight True\n",
      "transformer.h.6.mlp.c_fc.bias True\n",
      "transformer.h.6.mlp.c_proj.weight True\n",
      "transformer.h.6.mlp.c_proj.bias True\n",
      "transformer.h.7.ln_1.weight True\n",
      "transformer.h.7.ln_1.bias True\n",
      "transformer.h.7.attn.attention.k_proj.weight True\n",
      "transformer.h.7.attn.attention.v_proj.weight True\n",
      "transformer.h.7.attn.attention.q_proj.weight True\n",
      "transformer.h.7.attn.attention.out_proj.weight True\n",
      "transformer.h.7.attn.attention.out_proj.bias True\n",
      "transformer.h.7.ln_2.weight True\n",
      "transformer.h.7.ln_2.bias True\n",
      "transformer.h.7.mlp.c_fc.weight True\n",
      "transformer.h.7.mlp.c_fc.bias True\n",
      "transformer.h.7.mlp.c_proj.weight True\n",
      "transformer.h.7.mlp.c_proj.bias True\n",
      "transformer.h.8.ln_1.weight True\n",
      "transformer.h.8.ln_1.bias True\n",
      "transformer.h.8.attn.attention.k_proj.weight True\n",
      "transformer.h.8.attn.attention.v_proj.weight True\n",
      "transformer.h.8.attn.attention.q_proj.weight True\n",
      "transformer.h.8.attn.attention.out_proj.weight True\n",
      "transformer.h.8.attn.attention.out_proj.bias True\n",
      "transformer.h.8.ln_2.weight True\n",
      "transformer.h.8.ln_2.bias True\n",
      "transformer.h.8.mlp.c_fc.weight True\n",
      "transformer.h.8.mlp.c_fc.bias True\n",
      "transformer.h.8.mlp.c_proj.weight True\n",
      "transformer.h.8.mlp.c_proj.bias True\n",
      "transformer.h.9.ln_1.weight True\n",
      "transformer.h.9.ln_1.bias True\n",
      "transformer.h.9.attn.attention.k_proj.weight True\n",
      "transformer.h.9.attn.attention.v_proj.weight True\n",
      "transformer.h.9.attn.attention.q_proj.weight True\n",
      "transformer.h.9.attn.attention.out_proj.weight True\n",
      "transformer.h.9.attn.attention.out_proj.bias True\n",
      "transformer.h.9.ln_2.weight True\n",
      "transformer.h.9.ln_2.bias True\n",
      "transformer.h.9.mlp.c_fc.weight True\n",
      "transformer.h.9.mlp.c_fc.bias True\n",
      "transformer.h.9.mlp.c_proj.weight True\n",
      "transformer.h.9.mlp.c_proj.bias True\n",
      "transformer.h.10.ln_1.weight True\n",
      "transformer.h.10.ln_1.bias True\n",
      "transformer.h.10.attn.attention.k_proj.weight True\n",
      "transformer.h.10.attn.attention.v_proj.weight True\n",
      "transformer.h.10.attn.attention.q_proj.weight True\n",
      "transformer.h.10.attn.attention.out_proj.weight True\n",
      "transformer.h.10.attn.attention.out_proj.bias True\n",
      "transformer.h.10.ln_2.weight True\n",
      "transformer.h.10.ln_2.bias True\n",
      "transformer.h.10.mlp.c_fc.weight True\n",
      "transformer.h.10.mlp.c_fc.bias True\n",
      "transformer.h.10.mlp.c_proj.weight True\n",
      "transformer.h.10.mlp.c_proj.bias True\n",
      "transformer.h.11.ln_1.weight True\n",
      "transformer.h.11.ln_1.bias True\n",
      "transformer.h.11.attn.attention.k_proj.weight True\n",
      "transformer.h.11.attn.attention.v_proj.weight True\n",
      "transformer.h.11.attn.attention.q_proj.weight True\n",
      "transformer.h.11.attn.attention.out_proj.weight True\n",
      "transformer.h.11.attn.attention.out_proj.bias True\n",
      "transformer.h.11.ln_2.weight True\n",
      "transformer.h.11.ln_2.bias True\n",
      "transformer.h.11.mlp.c_fc.weight True\n",
      "transformer.h.11.mlp.c_fc.bias True\n",
      "transformer.h.11.mlp.c_proj.weight True\n",
      "transformer.h.11.mlp.c_proj.bias True\n",
      "transformer.h.12.ln_1.weight True\n",
      "transformer.h.12.ln_1.bias True\n",
      "transformer.h.12.attn.attention.k_proj.weight True\n",
      "transformer.h.12.attn.attention.v_proj.weight True\n",
      "transformer.h.12.attn.attention.q_proj.weight True\n",
      "transformer.h.12.attn.attention.out_proj.weight True\n",
      "transformer.h.12.attn.attention.out_proj.bias True\n",
      "transformer.h.12.ln_2.weight True\n",
      "transformer.h.12.ln_2.bias True\n",
      "transformer.h.12.mlp.c_fc.weight True\n",
      "transformer.h.12.mlp.c_fc.bias True\n",
      "transformer.h.12.mlp.c_proj.weight True\n",
      "transformer.h.12.mlp.c_proj.bias True\n",
      "transformer.h.13.ln_1.weight True\n",
      "transformer.h.13.ln_1.bias True\n",
      "transformer.h.13.attn.attention.k_proj.weight True\n",
      "transformer.h.13.attn.attention.v_proj.weight True\n",
      "transformer.h.13.attn.attention.q_proj.weight True\n",
      "transformer.h.13.attn.attention.out_proj.weight True\n",
      "transformer.h.13.attn.attention.out_proj.bias True\n",
      "transformer.h.13.ln_2.weight True\n",
      "transformer.h.13.ln_2.bias True\n",
      "transformer.h.13.mlp.c_fc.weight True\n",
      "transformer.h.13.mlp.c_fc.bias True\n",
      "transformer.h.13.mlp.c_proj.weight True\n",
      "transformer.h.13.mlp.c_proj.bias True\n",
      "transformer.h.14.ln_1.weight True\n",
      "transformer.h.14.ln_1.bias True\n",
      "transformer.h.14.attn.attention.k_proj.weight True\n",
      "transformer.h.14.attn.attention.v_proj.weight True\n",
      "transformer.h.14.attn.attention.q_proj.weight True\n",
      "transformer.h.14.attn.attention.out_proj.weight True\n",
      "transformer.h.14.attn.attention.out_proj.bias True\n",
      "transformer.h.14.ln_2.weight True\n",
      "transformer.h.14.ln_2.bias True\n",
      "transformer.h.14.mlp.c_fc.weight True\n",
      "transformer.h.14.mlp.c_fc.bias True\n",
      "transformer.h.14.mlp.c_proj.weight True\n",
      "transformer.h.14.mlp.c_proj.bias True\n",
      "transformer.h.15.ln_1.weight True\n",
      "transformer.h.15.ln_1.bias True\n",
      "transformer.h.15.attn.attention.k_proj.weight True\n",
      "transformer.h.15.attn.attention.v_proj.weight True\n",
      "transformer.h.15.attn.attention.q_proj.weight True\n",
      "transformer.h.15.attn.attention.out_proj.weight True\n",
      "transformer.h.15.attn.attention.out_proj.bias True\n",
      "transformer.h.15.ln_2.weight True\n",
      "transformer.h.15.ln_2.bias True\n",
      "transformer.h.15.mlp.c_fc.weight True\n",
      "transformer.h.15.mlp.c_fc.bias True\n",
      "transformer.h.15.mlp.c_proj.weight True\n",
      "transformer.h.15.mlp.c_proj.bias True\n",
      "transformer.h.16.ln_1.weight True\n",
      "transformer.h.16.ln_1.bias True\n",
      "transformer.h.16.attn.attention.k_proj.weight True\n",
      "transformer.h.16.attn.attention.v_proj.weight True\n",
      "transformer.h.16.attn.attention.q_proj.weight True\n",
      "transformer.h.16.attn.attention.out_proj.weight True\n",
      "transformer.h.16.attn.attention.out_proj.bias True\n",
      "transformer.h.16.ln_2.weight True\n",
      "transformer.h.16.ln_2.bias True\n",
      "transformer.h.16.mlp.c_fc.weight True\n",
      "transformer.h.16.mlp.c_fc.bias True\n",
      "transformer.h.16.mlp.c_proj.weight True\n",
      "transformer.h.16.mlp.c_proj.bias True\n",
      "transformer.h.17.ln_1.weight True\n",
      "transformer.h.17.ln_1.bias True\n",
      "transformer.h.17.attn.attention.k_proj.weight True\n",
      "transformer.h.17.attn.attention.v_proj.weight True\n",
      "transformer.h.17.attn.attention.q_proj.weight True\n",
      "transformer.h.17.attn.attention.out_proj.weight True\n",
      "transformer.h.17.attn.attention.out_proj.bias True\n",
      "transformer.h.17.ln_2.weight True\n",
      "transformer.h.17.ln_2.bias True\n",
      "transformer.h.17.mlp.c_fc.weight True\n",
      "transformer.h.17.mlp.c_fc.bias True\n",
      "transformer.h.17.mlp.c_proj.weight True\n",
      "transformer.h.17.mlp.c_proj.bias True\n",
      "transformer.h.18.ln_1.weight True\n",
      "transformer.h.18.ln_1.bias True\n",
      "transformer.h.18.attn.attention.k_proj.weight True\n",
      "transformer.h.18.attn.attention.v_proj.weight True\n",
      "transformer.h.18.attn.attention.q_proj.weight True\n",
      "transformer.h.18.attn.attention.out_proj.weight True\n",
      "transformer.h.18.attn.attention.out_proj.bias True\n",
      "transformer.h.18.ln_2.weight True\n",
      "transformer.h.18.ln_2.bias True\n",
      "transformer.h.18.mlp.c_fc.weight True\n",
      "transformer.h.18.mlp.c_fc.bias True\n",
      "transformer.h.18.mlp.c_proj.weight True\n",
      "transformer.h.18.mlp.c_proj.bias True\n",
      "transformer.h.19.ln_1.weight True\n",
      "transformer.h.19.ln_1.bias True\n",
      "transformer.h.19.attn.attention.k_proj.weight True\n",
      "transformer.h.19.attn.attention.v_proj.weight True\n",
      "transformer.h.19.attn.attention.q_proj.weight True\n",
      "transformer.h.19.attn.attention.out_proj.weight True\n",
      "transformer.h.19.attn.attention.out_proj.bias True\n",
      "transformer.h.19.ln_2.weight True\n",
      "transformer.h.19.ln_2.bias True\n",
      "transformer.h.19.mlp.c_fc.weight True\n",
      "transformer.h.19.mlp.c_fc.bias True\n",
      "transformer.h.19.mlp.c_proj.weight True\n",
      "transformer.h.19.mlp.c_proj.bias True\n",
      "transformer.h.20.ln_1.weight True\n",
      "transformer.h.20.ln_1.bias True\n",
      "transformer.h.20.attn.attention.k_proj.weight True\n",
      "transformer.h.20.attn.attention.v_proj.weight True\n",
      "transformer.h.20.attn.attention.q_proj.weight True\n",
      "transformer.h.20.attn.attention.out_proj.weight True\n",
      "transformer.h.20.attn.attention.out_proj.bias True\n",
      "transformer.h.20.ln_2.weight True\n",
      "transformer.h.20.ln_2.bias True\n",
      "transformer.h.20.mlp.c_fc.weight True\n",
      "transformer.h.20.mlp.c_fc.bias True\n",
      "transformer.h.20.mlp.c_proj.weight True\n",
      "transformer.h.20.mlp.c_proj.bias True\n",
      "transformer.h.21.ln_1.weight True\n",
      "transformer.h.21.ln_1.bias True\n",
      "transformer.h.21.attn.attention.k_proj.weight True\n",
      "transformer.h.21.attn.attention.v_proj.weight True\n",
      "transformer.h.21.attn.attention.q_proj.weight True\n",
      "transformer.h.21.attn.attention.out_proj.weight True\n",
      "transformer.h.21.attn.attention.out_proj.bias True\n",
      "transformer.h.21.ln_2.weight True\n",
      "transformer.h.21.ln_2.bias True\n",
      "transformer.h.21.mlp.c_fc.weight True\n",
      "transformer.h.21.mlp.c_fc.bias True\n",
      "transformer.h.21.mlp.c_proj.weight True\n",
      "transformer.h.21.mlp.c_proj.bias True\n",
      "transformer.h.22.ln_1.weight True\n",
      "transformer.h.22.ln_1.bias True\n",
      "transformer.h.22.attn.attention.k_proj.weight True\n",
      "transformer.h.22.attn.attention.v_proj.weight True\n",
      "transformer.h.22.attn.attention.q_proj.weight True\n",
      "transformer.h.22.attn.attention.out_proj.weight True\n",
      "transformer.h.22.attn.attention.out_proj.bias True\n",
      "transformer.h.22.ln_2.weight True\n",
      "transformer.h.22.ln_2.bias True\n",
      "transformer.h.22.mlp.c_fc.weight True\n",
      "transformer.h.22.mlp.c_fc.bias True\n",
      "transformer.h.22.mlp.c_proj.weight True\n",
      "transformer.h.22.mlp.c_proj.bias True\n",
      "transformer.h.23.ln_1.weight True\n",
      "transformer.h.23.ln_1.bias True\n",
      "transformer.h.23.attn.attention.k_proj.weight True\n",
      "transformer.h.23.attn.attention.v_proj.weight True\n",
      "transformer.h.23.attn.attention.q_proj.weight True\n",
      "transformer.h.23.attn.attention.out_proj.weight True\n",
      "transformer.h.23.attn.attention.out_proj.bias True\n",
      "transformer.h.23.ln_2.weight True\n",
      "transformer.h.23.ln_2.bias True\n",
      "transformer.h.23.mlp.c_fc.weight True\n",
      "transformer.h.23.mlp.c_fc.bias True\n",
      "transformer.h.23.mlp.c_proj.weight True\n",
      "transformer.h.23.mlp.c_proj.bias True\n",
      "transformer.ln_f.weight True\n",
      "transformer.ln_f.bias True\n"
     ]
    }
   ],
   "source": [
    "for name, param in model_neo.named_parameters():\n",
    "    print(name, param.requires_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# freeze the first two layers and 23 hidden units\n",
    "for name, param in model_neo.named_parameters():\n",
    "    if name.startswith(\"transformer.wte\"):\n",
    "        param.requires_grad = False\n",
    "    if name.startswith(\"transformer.wpe\"):\n",
    "        param.requires_grad = False\n",
    "    if any(x in name for x in ['.' + str(x) + '.' for x in range(23)]):\n",
    "        param.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "transformer.wte.weight False\n",
      "transformer.wpe.weight False\n",
      "transformer.h.0.ln_1.weight False\n",
      "transformer.h.0.ln_1.bias False\n",
      "transformer.h.0.attn.attention.k_proj.weight False\n",
      "transformer.h.0.attn.attention.v_proj.weight False\n",
      "transformer.h.0.attn.attention.q_proj.weight False\n",
      "transformer.h.0.attn.attention.out_proj.weight False\n",
      "transformer.h.0.attn.attention.out_proj.bias False\n",
      "transformer.h.0.ln_2.weight False\n",
      "transformer.h.0.ln_2.bias False\n",
      "transformer.h.0.mlp.c_fc.weight False\n",
      "transformer.h.0.mlp.c_fc.bias False\n",
      "transformer.h.0.mlp.c_proj.weight False\n",
      "transformer.h.0.mlp.c_proj.bias False\n",
      "transformer.h.1.ln_1.weight False\n",
      "transformer.h.1.ln_1.bias False\n",
      "transformer.h.1.attn.attention.k_proj.weight False\n",
      "transformer.h.1.attn.attention.v_proj.weight False\n",
      "transformer.h.1.attn.attention.q_proj.weight False\n",
      "transformer.h.1.attn.attention.out_proj.weight False\n",
      "transformer.h.1.attn.attention.out_proj.bias False\n",
      "transformer.h.1.ln_2.weight False\n",
      "transformer.h.1.ln_2.bias False\n",
      "transformer.h.1.mlp.c_fc.weight False\n",
      "transformer.h.1.mlp.c_fc.bias False\n",
      "transformer.h.1.mlp.c_proj.weight False\n",
      "transformer.h.1.mlp.c_proj.bias False\n",
      "transformer.h.2.ln_1.weight False\n",
      "transformer.h.2.ln_1.bias False\n",
      "transformer.h.2.attn.attention.k_proj.weight False\n",
      "transformer.h.2.attn.attention.v_proj.weight False\n",
      "transformer.h.2.attn.attention.q_proj.weight False\n",
      "transformer.h.2.attn.attention.out_proj.weight False\n",
      "transformer.h.2.attn.attention.out_proj.bias False\n",
      "transformer.h.2.ln_2.weight False\n",
      "transformer.h.2.ln_2.bias False\n",
      "transformer.h.2.mlp.c_fc.weight False\n",
      "transformer.h.2.mlp.c_fc.bias False\n",
      "transformer.h.2.mlp.c_proj.weight False\n",
      "transformer.h.2.mlp.c_proj.bias False\n",
      "transformer.h.3.ln_1.weight False\n",
      "transformer.h.3.ln_1.bias False\n",
      "transformer.h.3.attn.attention.k_proj.weight False\n",
      "transformer.h.3.attn.attention.v_proj.weight False\n",
      "transformer.h.3.attn.attention.q_proj.weight False\n",
      "transformer.h.3.attn.attention.out_proj.weight False\n",
      "transformer.h.3.attn.attention.out_proj.bias False\n",
      "transformer.h.3.ln_2.weight False\n",
      "transformer.h.3.ln_2.bias False\n",
      "transformer.h.3.mlp.c_fc.weight False\n",
      "transformer.h.3.mlp.c_fc.bias False\n",
      "transformer.h.3.mlp.c_proj.weight False\n",
      "transformer.h.3.mlp.c_proj.bias False\n",
      "transformer.h.4.ln_1.weight False\n",
      "transformer.h.4.ln_1.bias False\n",
      "transformer.h.4.attn.attention.k_proj.weight False\n",
      "transformer.h.4.attn.attention.v_proj.weight False\n",
      "transformer.h.4.attn.attention.q_proj.weight False\n",
      "transformer.h.4.attn.attention.out_proj.weight False\n",
      "transformer.h.4.attn.attention.out_proj.bias False\n",
      "transformer.h.4.ln_2.weight False\n",
      "transformer.h.4.ln_2.bias False\n",
      "transformer.h.4.mlp.c_fc.weight False\n",
      "transformer.h.4.mlp.c_fc.bias False\n",
      "transformer.h.4.mlp.c_proj.weight False\n",
      "transformer.h.4.mlp.c_proj.bias False\n",
      "transformer.h.5.ln_1.weight False\n",
      "transformer.h.5.ln_1.bias False\n",
      "transformer.h.5.attn.attention.k_proj.weight False\n",
      "transformer.h.5.attn.attention.v_proj.weight False\n",
      "transformer.h.5.attn.attention.q_proj.weight False\n",
      "transformer.h.5.attn.attention.out_proj.weight False\n",
      "transformer.h.5.attn.attention.out_proj.bias False\n",
      "transformer.h.5.ln_2.weight False\n",
      "transformer.h.5.ln_2.bias False\n",
      "transformer.h.5.mlp.c_fc.weight False\n",
      "transformer.h.5.mlp.c_fc.bias False\n",
      "transformer.h.5.mlp.c_proj.weight False\n",
      "transformer.h.5.mlp.c_proj.bias False\n",
      "transformer.h.6.ln_1.weight False\n",
      "transformer.h.6.ln_1.bias False\n",
      "transformer.h.6.attn.attention.k_proj.weight False\n",
      "transformer.h.6.attn.attention.v_proj.weight False\n",
      "transformer.h.6.attn.attention.q_proj.weight False\n",
      "transformer.h.6.attn.attention.out_proj.weight False\n",
      "transformer.h.6.attn.attention.out_proj.bias False\n",
      "transformer.h.6.ln_2.weight False\n",
      "transformer.h.6.ln_2.bias False\n",
      "transformer.h.6.mlp.c_fc.weight False\n",
      "transformer.h.6.mlp.c_fc.bias False\n",
      "transformer.h.6.mlp.c_proj.weight False\n",
      "transformer.h.6.mlp.c_proj.bias False\n",
      "transformer.h.7.ln_1.weight False\n",
      "transformer.h.7.ln_1.bias False\n",
      "transformer.h.7.attn.attention.k_proj.weight False\n",
      "transformer.h.7.attn.attention.v_proj.weight False\n",
      "transformer.h.7.attn.attention.q_proj.weight False\n",
      "transformer.h.7.attn.attention.out_proj.weight False\n",
      "transformer.h.7.attn.attention.out_proj.bias False\n",
      "transformer.h.7.ln_2.weight False\n",
      "transformer.h.7.ln_2.bias False\n",
      "transformer.h.7.mlp.c_fc.weight False\n",
      "transformer.h.7.mlp.c_fc.bias False\n",
      "transformer.h.7.mlp.c_proj.weight False\n",
      "transformer.h.7.mlp.c_proj.bias False\n",
      "transformer.h.8.ln_1.weight False\n",
      "transformer.h.8.ln_1.bias False\n",
      "transformer.h.8.attn.attention.k_proj.weight False\n",
      "transformer.h.8.attn.attention.v_proj.weight False\n",
      "transformer.h.8.attn.attention.q_proj.weight False\n",
      "transformer.h.8.attn.attention.out_proj.weight False\n",
      "transformer.h.8.attn.attention.out_proj.bias False\n",
      "transformer.h.8.ln_2.weight False\n",
      "transformer.h.8.ln_2.bias False\n",
      "transformer.h.8.mlp.c_fc.weight False\n",
      "transformer.h.8.mlp.c_fc.bias False\n",
      "transformer.h.8.mlp.c_proj.weight False\n",
      "transformer.h.8.mlp.c_proj.bias False\n",
      "transformer.h.9.ln_1.weight False\n",
      "transformer.h.9.ln_1.bias False\n",
      "transformer.h.9.attn.attention.k_proj.weight False\n",
      "transformer.h.9.attn.attention.v_proj.weight False\n",
      "transformer.h.9.attn.attention.q_proj.weight False\n",
      "transformer.h.9.attn.attention.out_proj.weight False\n",
      "transformer.h.9.attn.attention.out_proj.bias False\n",
      "transformer.h.9.ln_2.weight False\n",
      "transformer.h.9.ln_2.bias False\n",
      "transformer.h.9.mlp.c_fc.weight False\n",
      "transformer.h.9.mlp.c_fc.bias False\n",
      "transformer.h.9.mlp.c_proj.weight False\n",
      "transformer.h.9.mlp.c_proj.bias False\n",
      "transformer.h.10.ln_1.weight False\n",
      "transformer.h.10.ln_1.bias False\n",
      "transformer.h.10.attn.attention.k_proj.weight False\n",
      "transformer.h.10.attn.attention.v_proj.weight False\n",
      "transformer.h.10.attn.attention.q_proj.weight False\n",
      "transformer.h.10.attn.attention.out_proj.weight False\n",
      "transformer.h.10.attn.attention.out_proj.bias False\n",
      "transformer.h.10.ln_2.weight False\n",
      "transformer.h.10.ln_2.bias False\n",
      "transformer.h.10.mlp.c_fc.weight False\n",
      "transformer.h.10.mlp.c_fc.bias False\n",
      "transformer.h.10.mlp.c_proj.weight False\n",
      "transformer.h.10.mlp.c_proj.bias False\n",
      "transformer.h.11.ln_1.weight False\n",
      "transformer.h.11.ln_1.bias False\n",
      "transformer.h.11.attn.attention.k_proj.weight False\n",
      "transformer.h.11.attn.attention.v_proj.weight False\n",
      "transformer.h.11.attn.attention.q_proj.weight False\n",
      "transformer.h.11.attn.attention.out_proj.weight False\n",
      "transformer.h.11.attn.attention.out_proj.bias False\n",
      "transformer.h.11.ln_2.weight False\n",
      "transformer.h.11.ln_2.bias False\n",
      "transformer.h.11.mlp.c_fc.weight False\n",
      "transformer.h.11.mlp.c_fc.bias False\n",
      "transformer.h.11.mlp.c_proj.weight False\n",
      "transformer.h.11.mlp.c_proj.bias False\n",
      "transformer.h.12.ln_1.weight False\n",
      "transformer.h.12.ln_1.bias False\n",
      "transformer.h.12.attn.attention.k_proj.weight False\n",
      "transformer.h.12.attn.attention.v_proj.weight False\n",
      "transformer.h.12.attn.attention.q_proj.weight False\n",
      "transformer.h.12.attn.attention.out_proj.weight False\n",
      "transformer.h.12.attn.attention.out_proj.bias False\n",
      "transformer.h.12.ln_2.weight False\n",
      "transformer.h.12.ln_2.bias False\n",
      "transformer.h.12.mlp.c_fc.weight False\n",
      "transformer.h.12.mlp.c_fc.bias False\n",
      "transformer.h.12.mlp.c_proj.weight False\n",
      "transformer.h.12.mlp.c_proj.bias False\n",
      "transformer.h.13.ln_1.weight False\n",
      "transformer.h.13.ln_1.bias False\n",
      "transformer.h.13.attn.attention.k_proj.weight False\n",
      "transformer.h.13.attn.attention.v_proj.weight False\n",
      "transformer.h.13.attn.attention.q_proj.weight False\n",
      "transformer.h.13.attn.attention.out_proj.weight False\n",
      "transformer.h.13.attn.attention.out_proj.bias False\n",
      "transformer.h.13.ln_2.weight False\n",
      "transformer.h.13.ln_2.bias False\n",
      "transformer.h.13.mlp.c_fc.weight False\n",
      "transformer.h.13.mlp.c_fc.bias False\n",
      "transformer.h.13.mlp.c_proj.weight False\n",
      "transformer.h.13.mlp.c_proj.bias False\n",
      "transformer.h.14.ln_1.weight False\n",
      "transformer.h.14.ln_1.bias False\n",
      "transformer.h.14.attn.attention.k_proj.weight False\n",
      "transformer.h.14.attn.attention.v_proj.weight False\n",
      "transformer.h.14.attn.attention.q_proj.weight False\n",
      "transformer.h.14.attn.attention.out_proj.weight False\n",
      "transformer.h.14.attn.attention.out_proj.bias False\n",
      "transformer.h.14.ln_2.weight False\n",
      "transformer.h.14.ln_2.bias False\n",
      "transformer.h.14.mlp.c_fc.weight False\n",
      "transformer.h.14.mlp.c_fc.bias False\n",
      "transformer.h.14.mlp.c_proj.weight False\n",
      "transformer.h.14.mlp.c_proj.bias False\n",
      "transformer.h.15.ln_1.weight False\n",
      "transformer.h.15.ln_1.bias False\n",
      "transformer.h.15.attn.attention.k_proj.weight False\n",
      "transformer.h.15.attn.attention.v_proj.weight False\n",
      "transformer.h.15.attn.attention.q_proj.weight False\n",
      "transformer.h.15.attn.attention.out_proj.weight False\n",
      "transformer.h.15.attn.attention.out_proj.bias False\n",
      "transformer.h.15.ln_2.weight False\n",
      "transformer.h.15.ln_2.bias False\n",
      "transformer.h.15.mlp.c_fc.weight False\n",
      "transformer.h.15.mlp.c_fc.bias False\n",
      "transformer.h.15.mlp.c_proj.weight False\n",
      "transformer.h.15.mlp.c_proj.bias False\n",
      "transformer.h.16.ln_1.weight False\n",
      "transformer.h.16.ln_1.bias False\n",
      "transformer.h.16.attn.attention.k_proj.weight False\n",
      "transformer.h.16.attn.attention.v_proj.weight False\n",
      "transformer.h.16.attn.attention.q_proj.weight False\n",
      "transformer.h.16.attn.attention.out_proj.weight False\n",
      "transformer.h.16.attn.attention.out_proj.bias False\n",
      "transformer.h.16.ln_2.weight False\n",
      "transformer.h.16.ln_2.bias False\n",
      "transformer.h.16.mlp.c_fc.weight False\n",
      "transformer.h.16.mlp.c_fc.bias False\n",
      "transformer.h.16.mlp.c_proj.weight False\n",
      "transformer.h.16.mlp.c_proj.bias False\n",
      "transformer.h.17.ln_1.weight False\n",
      "transformer.h.17.ln_1.bias False\n",
      "transformer.h.17.attn.attention.k_proj.weight False\n",
      "transformer.h.17.attn.attention.v_proj.weight False\n",
      "transformer.h.17.attn.attention.q_proj.weight False\n",
      "transformer.h.17.attn.attention.out_proj.weight False\n",
      "transformer.h.17.attn.attention.out_proj.bias False\n",
      "transformer.h.17.ln_2.weight False\n",
      "transformer.h.17.ln_2.bias False\n",
      "transformer.h.17.mlp.c_fc.weight False\n",
      "transformer.h.17.mlp.c_fc.bias False\n",
      "transformer.h.17.mlp.c_proj.weight False\n",
      "transformer.h.17.mlp.c_proj.bias False\n",
      "transformer.h.18.ln_1.weight False\n",
      "transformer.h.18.ln_1.bias False\n",
      "transformer.h.18.attn.attention.k_proj.weight False\n",
      "transformer.h.18.attn.attention.v_proj.weight False\n",
      "transformer.h.18.attn.attention.q_proj.weight False\n",
      "transformer.h.18.attn.attention.out_proj.weight False\n",
      "transformer.h.18.attn.attention.out_proj.bias False\n",
      "transformer.h.18.ln_2.weight False\n",
      "transformer.h.18.ln_2.bias False\n",
      "transformer.h.18.mlp.c_fc.weight False\n",
      "transformer.h.18.mlp.c_fc.bias False\n",
      "transformer.h.18.mlp.c_proj.weight False\n",
      "transformer.h.18.mlp.c_proj.bias False\n",
      "transformer.h.19.ln_1.weight False\n",
      "transformer.h.19.ln_1.bias False\n",
      "transformer.h.19.attn.attention.k_proj.weight False\n",
      "transformer.h.19.attn.attention.v_proj.weight False\n",
      "transformer.h.19.attn.attention.q_proj.weight False\n",
      "transformer.h.19.attn.attention.out_proj.weight False\n",
      "transformer.h.19.attn.attention.out_proj.bias False\n",
      "transformer.h.19.ln_2.weight False\n",
      "transformer.h.19.ln_2.bias False\n",
      "transformer.h.19.mlp.c_fc.weight False\n",
      "transformer.h.19.mlp.c_fc.bias False\n",
      "transformer.h.19.mlp.c_proj.weight False\n",
      "transformer.h.19.mlp.c_proj.bias False\n",
      "transformer.h.20.ln_1.weight False\n",
      "transformer.h.20.ln_1.bias False\n",
      "transformer.h.20.attn.attention.k_proj.weight False\n",
      "transformer.h.20.attn.attention.v_proj.weight False\n",
      "transformer.h.20.attn.attention.q_proj.weight False\n",
      "transformer.h.20.attn.attention.out_proj.weight False\n",
      "transformer.h.20.attn.attention.out_proj.bias False\n",
      "transformer.h.20.ln_2.weight False\n",
      "transformer.h.20.ln_2.bias False\n",
      "transformer.h.20.mlp.c_fc.weight False\n",
      "transformer.h.20.mlp.c_fc.bias False\n",
      "transformer.h.20.mlp.c_proj.weight False\n",
      "transformer.h.20.mlp.c_proj.bias False\n",
      "transformer.h.21.ln_1.weight False\n",
      "transformer.h.21.ln_1.bias False\n",
      "transformer.h.21.attn.attention.k_proj.weight False\n",
      "transformer.h.21.attn.attention.v_proj.weight False\n",
      "transformer.h.21.attn.attention.q_proj.weight False\n",
      "transformer.h.21.attn.attention.out_proj.weight False\n",
      "transformer.h.21.attn.attention.out_proj.bias False\n",
      "transformer.h.21.ln_2.weight False\n",
      "transformer.h.21.ln_2.bias False\n",
      "transformer.h.21.mlp.c_fc.weight False\n",
      "transformer.h.21.mlp.c_fc.bias False\n",
      "transformer.h.21.mlp.c_proj.weight False\n",
      "transformer.h.21.mlp.c_proj.bias False\n",
      "transformer.h.22.ln_1.weight False\n",
      "transformer.h.22.ln_1.bias False\n",
      "transformer.h.22.attn.attention.k_proj.weight False\n",
      "transformer.h.22.attn.attention.v_proj.weight False\n",
      "transformer.h.22.attn.attention.q_proj.weight False\n",
      "transformer.h.22.attn.attention.out_proj.weight False\n",
      "transformer.h.22.attn.attention.out_proj.bias False\n",
      "transformer.h.22.ln_2.weight False\n",
      "transformer.h.22.ln_2.bias False\n",
      "transformer.h.22.mlp.c_fc.weight False\n",
      "transformer.h.22.mlp.c_fc.bias False\n",
      "transformer.h.22.mlp.c_proj.weight False\n",
      "transformer.h.22.mlp.c_proj.bias False\n",
      "transformer.h.23.ln_1.weight True\n",
      "transformer.h.23.ln_1.bias True\n",
      "transformer.h.23.attn.attention.k_proj.weight True\n",
      "transformer.h.23.attn.attention.v_proj.weight True\n",
      "transformer.h.23.attn.attention.q_proj.weight True\n",
      "transformer.h.23.attn.attention.out_proj.weight True\n",
      "transformer.h.23.attn.attention.out_proj.bias True\n",
      "transformer.h.23.ln_2.weight True\n",
      "transformer.h.23.ln_2.bias True\n",
      "transformer.h.23.mlp.c_fc.weight True\n",
      "transformer.h.23.mlp.c_fc.bias True\n",
      "transformer.h.23.mlp.c_proj.weight True\n",
      "transformer.h.23.mlp.c_proj.bias True\n",
      "transformer.ln_f.weight True\n",
      "transformer.ln_f.bias True\n"
     ]
    }
   ],
   "source": [
    "# check layers\n",
    "for name, param in model_neo.named_parameters():\n",
    "    print(name, param.requires_grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Shayne Kaiser\\anaconda3\\envs\\poetryproject\\lib\\site-packages\\transformers\\optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 2776605\n",
      "  Num Epochs = 1\n",
      "  Instantaneous batch size per device = 10\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 10\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 277661\n",
      "  Number of trainable parameters = 50356224\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fa88b8d6f7e84973ac677e7a59b0d559",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/277661 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.8757, 'learning_rate': 4.8200978926782185e-05, 'epoch': 0.04}\n",
      "{'loss': 0.8238, 'learning_rate': 4.640015703166925e-05, 'epoch': 0.07}\n",
      "{'loss': 0.8112, 'learning_rate': 4.459933513655633e-05, 'epoch': 0.11}\n",
      "{'loss': 0.8019, 'learning_rate': 4.27985132414434e-05, 'epoch': 0.14}\n",
      "{'loss': 0.7945, 'learning_rate': 4.099769134633047e-05, 'epoch': 0.18}\n",
      "{'loss': 0.7902, 'learning_rate': 3.919686945121754e-05, 'epoch': 0.22}\n",
      "{'loss': 0.7844, 'learning_rate': 3.739604755610461e-05, 'epoch': 0.25}\n",
      "{'loss': 0.7811, 'learning_rate': 3.559522566099168e-05, 'epoch': 0.29}\n",
      "{'loss': 0.7778, 'learning_rate': 3.3794403765878745e-05, 'epoch': 0.32}\n",
      "{'loss': 0.7751, 'learning_rate': 3.199358187076582e-05, 'epoch': 0.36}\n",
      "{'loss': 0.7725, 'learning_rate': 3.019275997565289e-05, 'epoch': 0.4}\n",
      "{'loss': 0.7696, 'learning_rate': 2.839193808053996e-05, 'epoch': 0.43}\n",
      "{'loss': 0.7676, 'learning_rate': 2.659111618542703e-05, 'epoch': 0.47}\n",
      "{'loss': 0.7668, 'learning_rate': 2.47902942903141e-05, 'epoch': 0.5}\n",
      "{'loss': 0.7636, 'learning_rate': 2.298947239520117e-05, 'epoch': 0.54}\n",
      "{'loss': 0.7623, 'learning_rate': 2.118865050008824e-05, 'epoch': 0.58}\n",
      "{'loss': 0.7599, 'learning_rate': 1.938782860497531e-05, 'epoch': 0.61}\n",
      "{'loss': 0.7588, 'learning_rate': 1.7587006709862385e-05, 'epoch': 0.65}\n",
      "{'loss': 0.758, 'learning_rate': 1.5786184814749453e-05, 'epoch': 0.68}\n",
      "{'loss': 0.7573, 'learning_rate': 1.3985362919636521e-05, 'epoch': 0.72}\n",
      "{'loss': 0.7553, 'learning_rate': 1.2184541024523593e-05, 'epoch': 0.76}\n",
      "{'loss': 0.7539, 'learning_rate': 1.0383719129410665e-05, 'epoch': 0.79}\n",
      "{'loss': 0.7536, 'learning_rate': 8.582897234297733e-06, 'epoch': 0.83}\n",
      "{'loss': 0.7511, 'learning_rate': 6.782075339184804e-06, 'epoch': 0.86}\n",
      "{'loss': 0.752, 'learning_rate': 4.981253444071874e-06, 'epoch': 0.9}\n",
      "{'loss': 0.7505, 'learning_rate': 3.1804315489589452e-06, 'epoch': 0.94}\n",
      "{'loss': 0.7505, 'learning_rate': 1.3796096538460155e-06, 'epoch': 0.97}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 123748.8893, 'train_samples_per_second': 22.437, 'train_steps_per_second': 2.244, 'train_loss': 0.7741056002052638, 'epoch': 1.0}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=277661, training_loss=0.7741056002052638, metrics={'train_runtime': 123748.8893, 'train_samples_per_second': 22.437, 'train_steps_per_second': 2.244, 'train_loss': 0.7741056002052638, 'epoch': 1.0})"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Trainer(model=model_neo,  args=training_args_neo, train_dataset=train_dataset_neo, \n",
    "        eval_dataset=val_dataset_neo, data_collator=lambda data: {'input_ids': torch.stack([f[0] for f in data]),\n",
    "                                                              'attention_mask': torch.stack([f[1] for f in data]),\n",
    "                                                              'labels': torch.stack([f[0] for f in data])}).train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Configuration saved in ./Models/GPT-Neo\\config.json\n",
      "Model weights saved in ./Models/GPT-Neo\\pytorch_model.bin\n"
     ]
    }
   ],
   "source": [
    "# Save model and Tokenizer\n",
    "model_neo.save_pretrained(\"./Models/GPT-Neo\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "tokenizer config file saved in ./Models/GPT-Neo\\tokenizer_config.json\n",
      "Special tokens file saved in ./Models/GPT-Neo\\special_tokens_map.json\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('./Models/GPT-Neo\\\\tokenizer_config.json',\n",
       " './Models/GPT-Neo\\\\special_tokens_map.json',\n",
       " './Models/GPT-Neo\\\\vocab.json',\n",
       " './Models/GPT-Neo\\\\merges.txt',\n",
       " './Models/GPT-Neo\\\\added_tokens.json',\n",
       " './Models/GPT-Neo\\\\tokenizer.json')"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer_neo.save_pretrained(\"./Models/GPT-Neo\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading file vocab.json\n",
      "loading file merges.txt\n",
      "loading file tokenizer.json\n",
      "loading file added_tokens.json\n",
      "loading file special_tokens_map.json\n",
      "loading file tokenizer_config.json\n",
      "loading configuration file ./Models/GPT-Neo\\config.json\n",
      "Model config GPTNeoConfig {\n",
      "  \"_name_or_path\": \"./Models/GPT-Neo\",\n",
      "  \"activation_function\": \"gelu_new\",\n",
      "  \"architectures\": [\n",
      "    \"GPTNeoForCausalLM\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0,\n",
      "  \"attention_layers\": [\n",
      "    \"global\",\n",
      "    \"local\",\n",
      "    \"global\",\n",
      "    \"local\",\n",
      "    \"global\",\n",
      "    \"local\",\n",
      "    \"global\",\n",
      "    \"local\",\n",
      "    \"global\",\n",
      "    \"local\",\n",
      "    \"global\",\n",
      "    \"local\",\n",
      "    \"global\",\n",
      "    \"local\",\n",
      "    \"global\",\n",
      "    \"local\",\n",
      "    \"global\",\n",
      "    \"local\",\n",
      "    \"global\",\n",
      "    \"local\",\n",
      "    \"global\",\n",
      "    \"local\",\n",
      "    \"global\",\n",
      "    \"local\"\n",
      "  ],\n",
      "  \"attention_types\": [\n",
      "    [\n",
      "      [\n",
      "        \"global\",\n",
      "        \"local\"\n",
      "      ],\n",
      "      12\n",
      "    ]\n",
      "  ],\n",
      "  \"bos_token_id\": 50256,\n",
      "  \"embed_dropout\": 0,\n",
      "  \"eos_token_id\": 50256,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_size\": 2048,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": null,\n",
      "  \"layer_norm_epsilon\": 1e-05,\n",
      "  \"max_position_embeddings\": 2048,\n",
      "  \"model_type\": \"gpt_neo\",\n",
      "  \"num_heads\": 16,\n",
      "  \"num_layers\": 24,\n",
      "  \"resid_dropout\": 0,\n",
      "  \"summary_activation\": null,\n",
      "  \"summary_first_dropout\": 0.1,\n",
      "  \"summary_proj_to_labels\": true,\n",
      "  \"summary_type\": \"cls_index\",\n",
      "  \"summary_use_proj\": true,\n",
      "  \"task_specific_params\": {\n",
      "    \"text-generation\": {\n",
      "      \"do_sample\": true,\n",
      "      \"max_length\": 50,\n",
      "      \"temperature\": 0.9\n",
      "    }\n",
      "  },\n",
      "  \"tokenizer_class\": \"GPT2Tokenizer\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.24.0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50259,\n",
      "  \"window_size\": 256\n",
      "}\n",
      "\n",
      "loading weights file ./Models/GPT-Neo\\pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing GPTNeoForCausalLM.\n",
      "\n",
      "All the weights of GPTNeoForCausalLM were initialized from the model checkpoint at ./Models/GPT-Neo.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use GPTNeoForCausalLM for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "# load in saved model and tokenizer \n",
    "tokenizer_neo = AutoTokenizer.from_pretrained(\"./Models/GPT-Neo\")\n",
    "model_neo = AutoModelForCausalLM.from_pretrained(\"./Models/GPT-Neo\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.8 ('poetryproject')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "ed33b6c83ed5036cd92c69b03bba619a7b3d4b7bbbfa1b7f16f401f486e9f1b2"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
